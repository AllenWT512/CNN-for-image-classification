{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. CIFAR10 Dataset\n",
    "The CIFAR-10 dataset consists of 50,000 training images, 10,000 test images, 10 categories, with 6,000 images per category. The images are of size 32×32×3. The image below lists the 10 categories, with 10 randomly displayed images from each category"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision.datasets import CIFAR10\n",
    "from torchvision.transforms import Compose\n",
    "from torchvision.transforms import ToTensor\n",
    "from torch.utils.data import DataLoader\n",
    "import torch.nn as nn\n",
    "import torchvision\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import time\n",
    "import numpy as np\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "train = CIFAR10(root='data', train=True, download=True,transform=Compose([ToTensor()]))\n",
    "valid = CIFAR10(root='data', train=False, download=True,transform=Compose([ToTensor()]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of training samples: 50000\n",
      "Number of testing samples: 10000\n",
      "Dataset shape: torch.Size([3, 32, 32])\n",
      "Dataset classes: {'airplane': 0, 'automobile': 1, 'bird': 2, 'cat': 3, 'deer': 4, 'dog': 5, 'frog': 6, 'horse': 7, 'ship': 8, 'truck': 9}\n"
     ]
    }
   ],
   "source": [
    "# Number of datasets\n",
    "print('Number of training samples:', len(train.targets))\n",
    "print('Number of testing samples:', len(valid.targets))\n",
    "\n",
    "# Dataset shape\n",
    "print(\"Dataset shape:\", train[0][0].shape)\n",
    "\n",
    "# Dataset classes\n",
    "print(\"Dataset classes:\", train.class_to_idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([8, 3, 32, 32])\n",
      "tensor([2, 8, 0, 7, 4, 9, 6, 8])\n"
     ]
    }
   ],
   "source": [
    "train = CIFAR10(root='data', train=True, transform=Compose([ToTensor()]))\n",
    "dataloader = DataLoader(train, batch_size=8, shuffle=True)\n",
    "for x, y in dataloader:\n",
    "    print(x.shape)\n",
    "    print(y)\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Building the Image Classification Network\n",
    "  The network structure we are going to build is as follows:\n",
    "\n",
    "  Input shape: 32x32\n",
    "\n",
    "  First Convolutional Layer: Input 3 channels, output 6 channels, Kernel Size: 3x3\n",
    "\n",
    "  First Pooling Layer: Input 30x30, output 15x15, Kernel Size: 2x2, Stride: 2\n",
    "\n",
    "  Second Convolutional Layer: Input 6 channels, output 16 channels, Kernel Size: 3x3\n",
    "\n",
    "  Second Pooling Layer: Input 13x13, output 6x6, Kernel Size: 2x2, Stride: 2\n",
    "\n",
    "  First Fully Connected Layer: Input 576 dimensions, output 120 dimensions\n",
    "\n",
    "  Second Fully Connected Layer: Input 120 dimensions, output 84 dimensions\n",
    "\n",
    "  Output Layer: Input 84 dimensions, output 10 dimensions\n",
    "\n",
    "  We apply the ReLU activation function after each convolution operation to introduce non-linearity into the network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ImageClassification(nn.Module):\n",
    "\n",
    "\n",
    "    def __init__(self):\n",
    "\n",
    "        super(ImageClassification, self).__init__()\n",
    "\n",
    "        self.conv1 = nn.Conv2d(3, 6, stride=1, kernel_size=3)\n",
    "        self.pool1 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        self.conv2 = nn.Conv2d(6, 16, stride=1, kernel_size=3)\n",
    "        self.pool2 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "\n",
    "        self.linear1 = nn.Linear(576, 120)\n",
    "        self.linear2 = nn.Linear(120, 84)\n",
    "        self.out = nn.Linear(84, 10)\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "\n",
    "        x = F.relu(self.conv1(x))\n",
    "        x = self.pool1(x)\n",
    "\n",
    "        x = F.relu(self.conv2(x))\n",
    "        x = self.pool2(x)\n",
    "\n",
    "        x = x.reshape(x.size(0), -1)\n",
    "        x = F.relu(self.linear1(x))\n",
    "        x = F.relu(self.linear2(x))\n",
    "\n",
    "        return self.out(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Writing the Training Function\n",
    "\n",
    "For training, we use the multi-class cross-entropy loss function and the Adam optimizer. The implementation code is as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train():\n",
    "\n",
    "    # Load the CIFAR10 training set and convert it to a tensor\n",
    "    transform = Compose([ToTensor()])\n",
    "    cifar10 = torchvision.datasets.CIFAR10(root='data', train=True, download=True, transform=transform)\n",
    "\n",
    "    # Build the image classification model\n",
    "    model = ImageClassification()\n",
    "    # Define the loss function\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    # Define the optimizer\n",
    "    optimizer = optim.Adam(model.parameters(), lr=1e-3)\n",
    "    # Number of epochs\n",
    "    epochs = 100\n",
    "\n",
    "    for epoch_idx in range(epochs):\n",
    "\n",
    "        # Create the data loader\n",
    "        dataloader = DataLoader(cifar10, batch_size=8, shuffle=True)\n",
    "        # Total sample count\n",
    "        sam_num = 0\n",
    "        # Total loss\n",
    "        total_loss = 0.0\n",
    "        # Start time\n",
    "        start = time.time()\n",
    "        correct = 0\n",
    "\n",
    "        for x, y in dataloader:\n",
    "            # Pass input to the model\n",
    "            output = model(x)\n",
    "            # Compute the loss\n",
    "            loss = criterion(output, y)\n",
    "            # Zero gradients\n",
    "            optimizer.zero_grad()\n",
    "            # Backpropagation\n",
    "            loss.backward()\n",
    "            # Update parameters\n",
    "            optimizer.step()\n",
    "\n",
    "            correct += (torch.argmax(output, dim=-1) == y).sum()\n",
    "            total_loss += (loss.item() * len(y))\n",
    "            sam_num += len(y)\n",
    "\n",
    "        print('epoch:%2s loss:%.5f acc:%.2f time:%.2fs' %\n",
    "              (epoch_idx + 1,\n",
    "               total_loss / sam_num,\n",
    "               correct / sam_num,\n",
    "               time.time() - start))\n",
    "\n",
    "    # Save the model\n",
    "    torch.save(model.state_dict(), 'model/image_classification.bin')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "epoch: 1 loss:1.60087 acc:0.41 time:49.98s\n",
      "epoch: 2 loss:1.30874 acc:0.53 time:52.45s\n",
      "epoch: 3 loss:1.19858 acc:0.57 time:52.14s\n",
      "epoch: 4 loss:1.12035 acc:0.60 time:50.97s\n",
      "epoch: 5 loss:1.06235 acc:0.62 time:51.71s\n",
      "epoch: 6 loss:1.01503 acc:0.64 time:51.50s\n",
      "epoch: 7 loss:0.97992 acc:0.65 time:52.01s\n",
      "epoch: 8 loss:0.94423 acc:0.67 time:52.98s\n",
      "epoch: 9 loss:0.92218 acc:0.68 time:54.11s\n",
      "epoch:10 loss:0.89140 acc:0.68 time:55.02s\n",
      "epoch:11 loss:0.87472 acc:0.69 time:55.94s\n",
      "epoch:12 loss:0.85397 acc:0.70 time:55.45s\n",
      "epoch:13 loss:0.83217 acc:0.70 time:53.93s\n",
      "epoch:14 loss:0.81138 acc:0.71 time:53.90s\n",
      "epoch:15 loss:0.79646 acc:0.71 time:53.97s\n",
      "epoch:16 loss:0.77884 acc:0.72 time:53.84s\n",
      "epoch:17 loss:0.76344 acc:0.73 time:53.82s\n",
      "epoch:18 loss:0.75280 acc:0.73 time:53.44s\n",
      "epoch:19 loss:0.73790 acc:0.74 time:53.62s\n",
      "epoch:20 loss:0.72384 acc:0.74 time:53.63s\n",
      "epoch:21 loss:0.70968 acc:0.75 time:53.41s\n",
      "epoch:22 loss:0.70075 acc:0.75 time:53.23s\n",
      "epoch:23 loss:0.68481 acc:0.76 time:53.66s\n",
      "epoch:24 loss:0.67310 acc:0.76 time:53.50s\n",
      "epoch:25 loss:0.66259 acc:0.76 time:53.30s\n",
      "epoch:26 loss:0.65308 acc:0.76 time:53.41s\n",
      "epoch:27 loss:0.63960 acc:0.77 time:52.63s\n",
      "epoch:28 loss:0.63021 acc:0.77 time:53.95s\n",
      "epoch:29 loss:0.62242 acc:0.78 time:53.41s\n",
      "epoch:30 loss:0.61197 acc:0.78 time:53.62s\n",
      "epoch:31 loss:0.60626 acc:0.78 time:52.96s\n",
      "epoch:32 loss:0.59199 acc:0.79 time:53.36s\n",
      "epoch:33 loss:0.58791 acc:0.79 time:53.00s\n",
      "epoch:34 loss:0.58045 acc:0.79 time:53.41s\n",
      "epoch:35 loss:0.57292 acc:0.79 time:53.19s\n",
      "epoch:36 loss:0.56649 acc:0.80 time:53.17s\n",
      "epoch:37 loss:0.55960 acc:0.80 time:53.45s\n",
      "epoch:38 loss:0.55263 acc:0.80 time:508.49s\n",
      "epoch:39 loss:0.54809 acc:0.80 time:561.23s\n",
      "epoch:40 loss:0.54256 acc:0.80 time:559.71s\n",
      "epoch:41 loss:0.53395 acc:0.81 time:554.09s\n",
      "epoch:42 loss:0.52543 acc:0.81 time:566.05s\n",
      "epoch:43 loss:0.52754 acc:0.81 time:580.67s\n",
      "epoch:44 loss:0.52052 acc:0.81 time:560.33s\n",
      "epoch:45 loss:0.51245 acc:0.81 time:446.13s\n",
      "epoch:46 loss:0.50657 acc:0.82 time:56.00s\n",
      "epoch:47 loss:0.50460 acc:0.82 time:54.03s\n",
      "epoch:48 loss:0.50363 acc:0.82 time:53.50s\n",
      "epoch:49 loss:0.49912 acc:0.82 time:53.34s\n",
      "epoch:50 loss:0.49404 acc:0.82 time:58.19s\n",
      "epoch:51 loss:0.49159 acc:0.82 time:57.47s\n",
      "epoch:52 loss:0.48021 acc:0.83 time:55.20s\n",
      "epoch:53 loss:0.48353 acc:0.83 time:54.33s\n",
      "epoch:54 loss:0.47758 acc:0.83 time:54.48s\n",
      "epoch:55 loss:0.47989 acc:0.83 time:55.79s\n",
      "epoch:56 loss:0.46224 acc:0.83 time:55.90s\n",
      "epoch:57 loss:0.46941 acc:0.83 time:54.54s\n",
      "epoch:58 loss:0.45976 acc:0.83 time:54.90s\n",
      "epoch:59 loss:0.46467 acc:0.83 time:54.11s\n",
      "epoch:60 loss:0.45432 acc:0.83 time:55.15s\n",
      "epoch:61 loss:0.45369 acc:0.84 time:54.57s\n",
      "epoch:62 loss:0.44971 acc:0.84 time:54.50s\n",
      "epoch:63 loss:0.44810 acc:0.84 time:53.82s\n",
      "epoch:64 loss:0.44425 acc:0.84 time:53.62s\n",
      "epoch:65 loss:0.44011 acc:0.84 time:55.96s\n",
      "epoch:66 loss:0.44335 acc:0.84 time:56.36s\n",
      "epoch:67 loss:0.44088 acc:0.84 time:56.40s\n",
      "epoch:68 loss:0.43459 acc:0.84 time:56.10s\n",
      "epoch:69 loss:0.43052 acc:0.84 time:56.50s\n",
      "epoch:70 loss:0.43065 acc:0.84 time:55.82s\n",
      "epoch:71 loss:0.42721 acc:0.84 time:56.26s\n",
      "epoch:72 loss:0.42374 acc:0.85 time:57.47s\n",
      "epoch:73 loss:0.42746 acc:0.85 time:57.41s\n",
      "epoch:74 loss:0.42274 acc:0.85 time:57.29s\n",
      "epoch:75 loss:0.41660 acc:0.85 time:58.09s\n",
      "epoch:76 loss:0.41757 acc:0.85 time:56.41s\n",
      "epoch:77 loss:0.41913 acc:0.85 time:57.23s\n",
      "epoch:78 loss:0.41424 acc:0.85 time:58.87s\n",
      "epoch:79 loss:0.41038 acc:0.85 time:53.83s\n",
      "epoch:80 loss:0.41342 acc:0.85 time:56.11s\n",
      "epoch:81 loss:0.40481 acc:0.85 time:58.46s\n",
      "epoch:82 loss:0.40186 acc:0.85 time:54.71s\n",
      "epoch:83 loss:0.40834 acc:0.85 time:56.65s\n",
      "epoch:84 loss:0.40219 acc:0.85 time:54.49s\n",
      "epoch:85 loss:0.40336 acc:0.85 time:58.03s\n",
      "epoch:86 loss:0.39574 acc:0.85 time:56.44s\n",
      "epoch:87 loss:0.39251 acc:0.86 time:54.68s\n",
      "epoch:88 loss:0.39842 acc:0.86 time:55.13s\n",
      "epoch:89 loss:0.39594 acc:0.86 time:54.64s\n",
      "epoch:90 loss:0.39753 acc:0.86 time:55.85s\n",
      "epoch:91 loss:0.39021 acc:0.86 time:57.54s\n",
      "epoch:92 loss:0.38542 acc:0.86 time:56.95s\n",
      "epoch:93 loss:0.39184 acc:0.86 time:55.28s\n",
      "epoch:94 loss:0.38048 acc:0.86 time:55.22s\n",
      "epoch:95 loss:0.38372 acc:0.86 time:55.68s\n",
      "epoch:96 loss:0.38399 acc:0.86 time:54.66s\n",
      "epoch:97 loss:0.38300 acc:0.86 time:55.35s\n",
      "epoch:98 loss:0.37910 acc:0.86 time:56.32s\n",
      "epoch:99 loss:0.37283 acc:0.87 time:56.68s\n",
      "epoch:100 loss:0.37931 acc:0.86 time:56.63s\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Parent directory model does not exist.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[41], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[40], line 51\u001b[0m, in \u001b[0;36mtrain\u001b[1;34m()\u001b[0m\n\u001b[0;32m     44\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mepoch:\u001b[39m\u001b[38;5;132;01m%2s\u001b[39;00m\u001b[38;5;124m loss:\u001b[39m\u001b[38;5;132;01m%.5f\u001b[39;00m\u001b[38;5;124m acc:\u001b[39m\u001b[38;5;132;01m%.2f\u001b[39;00m\u001b[38;5;124m time:\u001b[39m\u001b[38;5;132;01m%.2f\u001b[39;00m\u001b[38;5;124ms\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;241m%\u001b[39m\n\u001b[0;32m     45\u001b[0m           (epoch_idx \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m,\n\u001b[0;32m     46\u001b[0m            total_loss \u001b[38;5;241m/\u001b[39m sam_num,\n\u001b[0;32m     47\u001b[0m            correct \u001b[38;5;241m/\u001b[39m sam_num,\n\u001b[0;32m     48\u001b[0m            time\u001b[38;5;241m.\u001b[39mtime() \u001b[38;5;241m-\u001b[39m start))\n\u001b[0;32m     50\u001b[0m \u001b[38;5;66;03m# Save the model\u001b[39;00m\n\u001b[1;32m---> 51\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msave\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstate_dict\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mmodel/image_classification.bin\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32md:\\Anaconda3\\lib\\site-packages\\torch\\serialization.py:618\u001b[0m, in \u001b[0;36msave\u001b[1;34m(obj, f, pickle_module, pickle_protocol, _use_new_zipfile_serialization, _disable_byteorder_record)\u001b[0m\n\u001b[0;32m    615\u001b[0m _check_save_filelike(f)\n\u001b[0;32m    617\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m _use_new_zipfile_serialization:\n\u001b[1;32m--> 618\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[43m_open_zipfile_writer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m opened_zipfile:\n\u001b[0;32m    619\u001b[0m         _save(obj, opened_zipfile, pickle_module, pickle_protocol, _disable_byteorder_record)\n\u001b[0;32m    620\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m\n",
      "File \u001b[1;32md:\\Anaconda3\\lib\\site-packages\\torch\\serialization.py:492\u001b[0m, in \u001b[0;36m_open_zipfile_writer\u001b[1;34m(name_or_buffer)\u001b[0m\n\u001b[0;32m    490\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    491\u001b[0m     container \u001b[38;5;241m=\u001b[39m _open_zipfile_writer_buffer\n\u001b[1;32m--> 492\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mcontainer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname_or_buffer\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32md:\\Anaconda3\\lib\\site-packages\\torch\\serialization.py:463\u001b[0m, in \u001b[0;36m_open_zipfile_writer_file.__init__\u001b[1;34m(self, name)\u001b[0m\n\u001b[0;32m    461\u001b[0m     \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m(torch\u001b[38;5;241m.\u001b[39m_C\u001b[38;5;241m.\u001b[39mPyTorchFileWriter(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfile_stream))\n\u001b[0;32m    462\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 463\u001b[0m     \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m(\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_C\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mPyTorchFileWriter\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m)\n",
      "\u001b[1;31mRuntimeError\u001b[0m: Parent directory model does not exist."
     ]
    }
   ],
   "source": [
    "train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. Writing the Prediction Function\n",
    "We load the trained model and make predictions on the 10,000 samples in the test set to evaluate the model's accuracy on the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test():\n",
    "\n",
    "    # Load the CIFAR10 test set and convert it to a tensor\n",
    "    transform = Compose([ToTensor()])\n",
    "    cifar10 = torchvision.datasets.CIFAR10(root='data', train=False, download=True, transform=transform)\n",
    "    # Create the data loader\n",
    "    dataloader = DataLoader(cifar10, batch_size=18, shuffle=True)\n",
    "    # Load the model\n",
    "    model = ImageClassification()\n",
    "    model.load_state_dict(torch.load('model/image_classification.bin'))\n",
    "    model.eval()\n",
    "\n",
    "    total_correct = 0\n",
    "    total_samples = 0\n",
    "    for x, y in dataloader:\n",
    "        output = model(x)\n",
    "        total_correct += (torch.argmax(output, dim=-1) == y).sum()\n",
    "        total_samples += len(y)\n",
    "\n",
    "    print('Acc: %.2f' % (total_correct / total_samples))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the results of the program's execution, the network model's accuracy on the test set is not very high. We can adjust the network in the following ways:\n",
    "\n",
    "Increase the number of output channels in the convolutional layers\n",
    "\n",
    "Increase the number of parameters in the fully connected layers\n",
    "\n",
    "Adjust the learning rate\n",
    "\n",
    "Change the optimization method\n",
    "\n",
    "Modify the activation function\n",
    "\n",
    "And so on...\n",
    "\n",
    "I modified the learning rate from 1e-3 to 1e-4, and increased the network's parameter count as shown in the code below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ImageClassification(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super(ImageClassification, self).__init__()\n",
    "\n",
    "        self.conv1 = nn.Conv2d(3, 32, stride=1, kernel_size=3)\n",
    "        self.pool1 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        self.conv2 = nn.Conv2d(32, 128, stride=1, kernel_size=3)\n",
    "        self.pool2 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "\n",
    "        self.linear1 = nn.Linear(128 * 6 * 6, 2048)\n",
    "        self.linear2 = nn.Linear(2048, 2048)\n",
    "        self.out = nn.Linear(2048, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.conv1(x))\n",
    "        x = self.pool1(x)\n",
    "\n",
    "        x = F.relu(self.conv2(x))\n",
    "        x = self.pool2(x)\n",
    "\n",
    "        # Since the last batch might not be a full 32, we need to flatten based on batch size\n",
    "        x = x.reshape(x.size(0), -1)\n",
    "\n",
    "        x = F.relu(self.linear1(x))\n",
    "        x = F.dropout(x, p=0.5)\n",
    "\n",
    "        x = F.relu(self.linear2(x))\n",
    "        x = F.dropout(x, p=0.5)\n",
    "\n",
    "        return self.out(x)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
